{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Documents\n",
    "\n",
    "Learn how to upload documents to Storm API for learning and RAG (Retrieval-Augmented Generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "API_KEY = os.getenv(\"STORM_API_KEY\", \"your-api-key-here\")\n",
    "API_URL = \"https://https://live-stargate.sionic.im\"\n",
    "\n",
    "headers = {\"storm-api-key\": API_KEY}\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload Methods Overview\n",
    "\n",
    "Storm API supports two document upload methods:\n",
    "- **File Upload**: Direct file upload from your local system\n",
    "- **URL Upload**: Upload from URLs (Google Drive, S3, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get agent and bucket IDs first\n",
    "response = requests.get(\n",
    "    f\"{API_URL}/api/v2/agents\",\n",
    "    headers=headers,\n",
    "    params={\"page\": 1, \"size\": 1}\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    agent = response.json()[\"data\"][\"data\"][0]\n",
    "    agent_id = agent[\"id\"]\n",
    "    print(f\"‚úÖ Using agent: {agent['name']}\")\n",
    "    \n",
    "    # Get first bucket\n",
    "    response = requests.get(\n",
    "        f\"{API_URL}/api/v2/buckets\",\n",
    "        headers=headers,\n",
    "        params={\"agentId\": agent_id, \"page\": 1, \"size\": 1}\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200 and response.json()[\"data\"][\"data\"]:\n",
    "        bucket = response.json()[\"data\"][\"data\"][0]\n",
    "        bucket_id = bucket[\"id\"]\n",
    "        print(f\"‚úÖ Using bucket: {bucket['name']}\")\n",
    "    else:\n",
    "        print(\"‚ùå No buckets found. Creating one...\")\n",
    "        response = requests.post(\n",
    "            f\"{API_URL}/api/v2/buckets\",\n",
    "            headers=headers,\n",
    "            json={\"agentId\": agent_id, \"name\": \"test-bucket\"}\n",
    "        )\n",
    "        bucket = response.json()[\"data\"]\n",
    "        bucket_id = bucket[\"id\"]\n",
    "        print(f\"‚úÖ Created bucket: {bucket['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Document by File\n",
    "\n",
    "Upload documents directly from your file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_document_by_file(bucket_id, file_path, parser_type=\"DEFAULT\", webhook_url=None):\n",
    "    \"\"\"Upload a document file to Storm API.\"\"\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare form data\n",
    "    data = {\n",
    "        \"bucketId\": bucket_id,\n",
    "        \"parserType\": parser_type\n",
    "    }\n",
    "    \n",
    "    if webhook_url:\n",
    "        data[\"webhookUrl\"] = webhook_url\n",
    "    \n",
    "    # Upload file\n",
    "    with open(file_path, 'rb') as f:\n",
    "        files = {\"file\": (os.path.basename(file_path), f)}\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{API_URL}/api/v2/documents/by-file\",\n",
    "            headers=headers,\n",
    "            data=data,\n",
    "            files=files\n",
    "        )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        document = response.json()[\"data\"]\n",
    "        print(f\"‚úÖ Document uploaded successfully!\")\n",
    "        print(f\"   ID: {document['id']}\")\n",
    "        print(f\"   Name: {document['name']}\")\n",
    "        print(f\"   Status: {document['status']}\")\n",
    "        print(f\"   Parser: {document['parserType']}\")\n",
    "        return document\n",
    "    else:\n",
    "        print(f\"‚ùå Upload failed: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "# Create a sample file to upload\n",
    "sample_file = \"sample_document.txt\"\n",
    "with open(sample_file, \"w\") as f:\n",
    "    f.write(\"\"\"Storm API Document Upload Example\n",
    "\n",
    "This is a sample document to demonstrate the Storm API document upload functionality.\n",
    "\n",
    "Key Features:\n",
    "- Document learning with AI\n",
    "- Intelligent search and retrieval\n",
    "- Multi-format support\n",
    "- Real-time processing\n",
    "\n",
    "Storm API makes it easy to build RAG applications!\"\"\")\n",
    "\n",
    "# Upload the document\n",
    "if 'bucket_id' in locals():\n",
    "    uploaded_doc = upload_document_by_file(bucket_id, sample_file)\n",
    "    \n",
    "    # Clean up\n",
    "    os.remove(sample_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload Document by URL\n",
    "\n",
    "Upload documents from external URLs like Google Drive or S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_document_by_url(bucket_id, url, parser_type=\"DEFAULT\", webhook_url=None):\n",
    "    \"\"\"Upload a document from URL to Storm API.\"\"\"\n",
    "    \n",
    "    data = {\n",
    "        \"bucketId\": bucket_id,\n",
    "        \"url\": url,\n",
    "        \"parserType\": parser_type\n",
    "    }\n",
    "    \n",
    "    if webhook_url:\n",
    "        data[\"webhookUrl\"] = webhook_url\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{API_URL}/api/v2/documents/by-url\",\n",
    "        headers=headers,\n",
    "        json=data\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        document = response.json()[\"data\"]\n",
    "        print(f\"‚úÖ Document URL uploaded successfully!\")\n",
    "        print(f\"   ID: {document['id']}\")\n",
    "        print(f\"   Name: {document['name']}\")\n",
    "        print(f\"   Source: {document['source']}\")\n",
    "        print(f\"   Status: {document['status']}\")\n",
    "        return document\n",
    "    else:\n",
    "        print(f\"‚ùå Upload failed: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "# Example URLs (replace with actual URLs)\n",
    "example_urls = [\n",
    "    \"https://drive.google.com/file/d/YOUR_FILE_ID/view?usp=sharing\",\n",
    "    \"https://your-bucket.s3.amazonaws.com/document.pdf\"\n",
    "]\n",
    "\n",
    "print(\"üìå URL Upload Examples:\")\n",
    "print(\"\\nGoogle Drive:\")\n",
    "print(\"1. Make sure file is publicly accessible\")\n",
    "print(\"2. Use the sharing link\")\n",
    "print(\"\\nS3:\")\n",
    "print(\"1. File must be publicly readable\")\n",
    "print(\"2. Or use pre-signed URLs\")\n",
    "\n",
    "# Uncomment to test with a real URL\n",
    "# if 'bucket_id' in locals():\n",
    "#     url = \"YOUR_DOCUMENT_URL_HERE\"\n",
    "#     uploaded_url_doc = upload_document_by_url(bucket_id, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parser Types\n",
    "\n",
    "Storm API supports different parser types for document processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã Parser Types:\\n\")\n",
    "\n",
    "print(\"1. DEFAULT Parser:\")\n",
    "print(\"   ‚Ä¢ Fast processing (10-60 seconds)\")\n",
    "print(\"   ‚Ä¢ Standard text extraction\")\n",
    "print(\"   ‚Ä¢ Good for most documents\")\n",
    "print(\"   ‚Ä¢ Preserves basic formatting\")\n",
    "\n",
    "print(\"\\n2. STORM_PARSE Parser:\")\n",
    "print(\"   ‚Ä¢ Advanced processing (may take longer)\")\n",
    "print(\"   ‚Ä¢ Better handling of complex layouts\")\n",
    "print(\"   ‚Ä¢ Extracts tables and structured data\")\n",
    "print(\"   ‚Ä¢ Ideal for technical documents\")\n",
    "\n",
    "# Example: Compare parsers\n",
    "def compare_parsers(bucket_id, file_path):\n",
    "    \"\"\"Upload same document with different parsers.\"\"\"\n",
    "    parsers = [\"DEFAULT\", \"STORM_PARSE\"]\n",
    "    results = {}\n",
    "    \n",
    "    for parser in parsers:\n",
    "        print(f\"\\nTesting {parser} parser...\")\n",
    "        doc = upload_document_by_file(bucket_id, file_path, parser_type=parser)\n",
    "        if doc:\n",
    "            results[parser] = doc\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create a test document with complex formatting\n",
    "complex_file = \"complex_document.txt\"\n",
    "with open(complex_file, \"w\") as f:\n",
    "    f.write(\"\"\"Financial Report Q4 2024\n",
    "\n",
    "| Category | Q3 2024 | Q4 2024 | Change |\n",
    "|----------|---------|---------|--------|\n",
    "| Revenue  | $1.2M   | $1.5M   | +25%   |\n",
    "| Expenses | $800K   | $900K   | +12.5% |\n",
    "| Profit   | $400K   | $600K   | +50%   |\n",
    "\n",
    "Key Insights:\n",
    "‚Ä¢ Strong revenue growth driven by new product launches\n",
    "‚Ä¢ Controlled expense increase despite expansion\n",
    "‚Ä¢ Profit margin improved from 33% to 40%\n",
    "\"\"\")\n",
    "\n",
    "# Test different parsers\n",
    "# if 'bucket_id' in locals():\n",
    "#     parser_results = compare_parsers(bucket_id, complex_file)\n",
    "\n",
    "# Clean up\n",
    "os.remove(complex_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Monitor Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_document_status(document_id, agent_id, bucket_id):\n",
    "    \"\"\"Check the processing status of a document.\"\"\"\n",
    "    \n",
    "    response = requests.get(\n",
    "        f\"{API_URL}/api/v2/documents\",\n",
    "        headers=headers,\n",
    "        params={\n",
    "            \"agentId\": agent_id,\n",
    "            \"bucketId\": bucket_id,\n",
    "            \"documentId\": document_id\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        documents = response.json()[\"data\"][\"data\"]\n",
    "        if documents:\n",
    "            doc = documents[0]\n",
    "            return doc[\"status\"]\n",
    "    return None\n",
    "\n",
    "def wait_for_document_processing(document_id, agent_id, bucket_id, timeout=300):\n",
    "    \"\"\"Wait for document to finish processing.\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"‚è≥ Waiting for document to process...\", end=\"\")\n",
    "    \n",
    "    while time.time() - start_time < timeout:\n",
    "        status = check_document_status(document_id, agent_id, bucket_id)\n",
    "        \n",
    "        if status == \"completed\":\n",
    "            print(\" ‚úÖ Completed!\")\n",
    "            return True\n",
    "        elif status == \"failed\":\n",
    "            print(\" ‚ùå Failed!\")\n",
    "            return False\n",
    "        \n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(5)\n",
    "    \n",
    "    print(\" ‚è±Ô∏è Timeout!\")\n",
    "    return False\n",
    "\n",
    "# Monitor the uploaded document\n",
    "if 'uploaded_doc' in locals() and uploaded_doc:\n",
    "    doc_id = uploaded_doc['id']\n",
    "    print(f\"\\nMonitoring document: {doc_id}\")\n",
    "    \n",
    "    # Check initial status\n",
    "    print(f\"Initial status: {uploaded_doc['status']}\")\n",
    "    \n",
    "    # Wait for processing\n",
    "    if uploaded_doc['status'] == \"progress\":\n",
    "        success = wait_for_document_processing(doc_id, agent_id, bucket_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def batch_upload_documents(bucket_id, file_paths, max_workers=3):\n",
    "    \"\"\"Upload multiple documents concurrently.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all uploads\n",
    "        future_to_file = {\n",
    "            executor.submit(upload_document_by_file, bucket_id, fp): fp\n",
    "            for fp in file_paths\n",
    "        }\n",
    "        \n",
    "        # Collect results\n",
    "        for future in concurrent.futures.as_completed(future_to_file):\n",
    "            file_path = future_to_file[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append({\n",
    "                    \"file\": file_path,\n",
    "                    \"success\": result is not None,\n",
    "                    \"document\": result\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"file\": file_path,\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "    \n",
    "    # Summary\n",
    "    successful = sum(1 for r in results if r[\"success\"])\n",
    "    print(f\"\\nüìä Batch Upload Summary:\")\n",
    "    print(f\"   ‚Ä¢ Total: {len(results)}\")\n",
    "    print(f\"   ‚Ä¢ Successful: {successful}\")\n",
    "    print(f\"   ‚Ä¢ Failed: {len(results) - successful}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create test files for batch upload\n",
    "test_files = []\n",
    "for i in range(3):\n",
    "    filename = f\"batch_test_{i+1}.txt\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(f\"\"\"Document {i+1}\n",
    "        \n",
    "This is test document number {i+1} for batch upload.\n",
    "It contains sample content for Storm API processing.\n",
    "\"\"\")\n",
    "    test_files.append(filename)\n",
    "\n",
    "print(f\"Created {len(test_files)} test files for batch upload\")\n",
    "\n",
    "# Perform batch upload\n",
    "if 'bucket_id' in locals():\n",
    "    batch_results = batch_upload_documents(bucket_id, test_files)\n",
    "    \n",
    "    # Show results\n",
    "    print(\"\\nüìÑ Individual Results:\")\n",
    "    for result in batch_results:\n",
    "        status = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "        print(f\"{status} {result['file']}\")\n",
    "\n",
    "# Clean up\n",
    "for f in test_files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Webhook Integration\n",
    "\n",
    "Use webhooks to get notified when document processing completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example webhook payload structure\n",
    "webhook_example = {\n",
    "    \"documentId\": \"1234567890\",\n",
    "    \"status\": \"completed\",\n",
    "    \"bucketId\": \"bucket-123\",\n",
    "    \"name\": \"document.pdf\",\n",
    "    \"characters\": 5000,\n",
    "    \"processingTime\": 45,\n",
    "    \"timestamp\": \"2024-01-01T12:00:00Z\"\n",
    "}\n",
    "\n",
    "print(\"ü™ù Webhook Integration:\\n\")\n",
    "print(\"1. Set up webhook endpoint:\")\n",
    "print(\"   POST https://your-server.com/webhooks/storm-document\")\n",
    "\n",
    "print(\"\\n2. Include webhook URL in upload:\")\n",
    "print(\"\"\"   upload_document_by_file(\n",
    "       bucket_id,\n",
    "       \"document.pdf\",\n",
    "       webhook_url=\"https://your-server.com/webhooks/storm-document\"\n",
    "   )\"\"\")\n",
    "\n",
    "print(\"\\n3. Expected webhook payload:\")\n",
    "print(json.dumps(webhook_example, indent=2))\n",
    "\n",
    "print(\"\\n4. Webhook handler example (Flask):\")\n",
    "print(\"\"\"@app.route('/webhooks/storm-document', methods=['POST'])\n",
    "def handle_storm_webhook():\n",
    "    data = request.json\n",
    "    \n",
    "    if data['status'] == 'completed':\n",
    "        # Document ready for use\n",
    "        process_completed_document(data['documentId'])\n",
    "    elif data['status'] == 'failed':\n",
    "        # Handle failure\n",
    "        log_failed_document(data['documentId'])\n",
    "    \n",
    "    return {'status': 'ok'}, 200\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìö Document Upload Best Practices:\\n\")\n",
    "\n",
    "print(\"1. File Size & Format:\")\n",
    "print(\"   ‚Ä¢ Maximum file size: 10 MB\")\n",
    "print(\"   ‚Ä¢ Supported formats: PDF, DOCX, TXT, etc.\")\n",
    "print(\"   ‚Ä¢ Compress large files before upload\")\n",
    "\n",
    "print(\"\\n2. Error Handling:\")\n",
    "print(\"   ‚Ä¢ Always check response status\")\n",
    "print(\"   ‚Ä¢ Implement retry logic for failures\")\n",
    "print(\"   ‚Ä¢ Log errors for debugging\")\n",
    "\n",
    "print(\"\\n3. Performance:\")\n",
    "print(\"   ‚Ä¢ Use batch upload for multiple files\")\n",
    "print(\"   ‚Ä¢ Implement progress tracking\")\n",
    "print(\"   ‚Ä¢ Use webhooks for async processing\")\n",
    "\n",
    "print(\"\\n4. Organization:\")\n",
    "print(\"   ‚Ä¢ Use meaningful file names\")\n",
    "print(\"   ‚Ä¢ Organize documents in appropriate buckets\")\n",
    "print(\"   ‚Ä¢ Add metadata when available\")\n",
    "\n",
    "# Example: Robust upload function\n",
    "def robust_upload(bucket_id, file_path, max_retries=3):\n",
    "    \"\"\"Upload with retry logic and error handling.\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Check file\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "            \n",
    "            # Check file size\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            if file_size > 10 * 1024 * 1024:  # 10 MB\n",
    "                raise ValueError(f\"File too large: {file_size / 1024 / 1024:.1f} MB\")\n",
    "            \n",
    "            # Upload\n",
    "            result = upload_document_by_file(bucket_id, file_path)\n",
    "            \n",
    "            if result:\n",
    "                return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"\\n‚úÖ Upload function with retry logic created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned how to:\n",
    "- ‚úÖ Upload documents by file and URL\n",
    "- ‚úÖ Use different parser types\n",
    "- ‚úÖ Monitor document processing\n",
    "- ‚úÖ Perform batch uploads\n",
    "- ‚úÖ Integrate webhooks\n",
    "- ‚úÖ Follow best practices\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Document Processing](./03-document-processing.md) - Learn about document learning\n",
    "- [Basic Chat](../04-chat-system/01-basic-chat.ipynb) - Start chatting with your documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}