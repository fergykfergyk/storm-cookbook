{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Chat Responses\n",
    "\n",
    "Learn how to use Storm API's streaming endpoint for real-time chat responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "API_KEY = os.getenv(\"STORM_API_KEY\", \"your-api-key-here\")\n",
    "API_URL = \"https://https://live-stargate.sionic.im\"\n",
    "\n",
    "headers = {\"storm-api-key\": API_KEY}\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Streaming\n",
    "\n",
    "Streaming provides real-time responses using Server-Sent Events (SSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üåä Streaming vs Non-Streaming:\\n\")\n",
    "\n",
    "print(\"Non-Streaming (/api/v2/answer):\")\n",
    "print(\"  ‚Ä¢ Wait for complete response\")\n",
    "print(\"  ‚Ä¢ Get answer all at once\")\n",
    "print(\"  ‚Ä¢ Simple to implement\")\n",
    "print(\"  ‚Ä¢ Good for short responses\")\n",
    "\n",
    "print(\"\\nStreaming (/api/v2/answer/stream):\")\n",
    "print(\"  ‚Ä¢ Get response in real-time\")\n",
    "print(\"  ‚Ä¢ Better user experience\")\n",
    "print(\"  ‚Ä¢ Show progress as it generates\")\n",
    "print(\"  ‚Ä¢ Ideal for long responses\")\n",
    "\n",
    "print(\"\\nüìã SSE Event Format:\")\n",
    "print(\"\"\"data: {\"status\": \"success\", \"data\": {...}}\n",
    "data: {\"status\": \"success\", \"data\": {...}}\n",
    "data: {\"status\": \"success\", \"data\": {\"is_final_event\": true}}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Streaming Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_chat(question, bucket_ids=None, thread_id=None):\n",
    "    \"\"\"Stream chat response from Storm API.\"\"\"\n",
    "    \n",
    "    data = {\n",
    "        \"question\": question,\n",
    "        \"bucketIds\": bucket_ids or [],\n",
    "    }\n",
    "    \n",
    "    if thread_id:\n",
    "        data[\"threadId\"] = thread_id\n",
    "    \n",
    "    # Make streaming request\n",
    "    response = requests.post(\n",
    "        f\"{API_URL}/api/v2/answer/stream\",\n",
    "        headers=headers,\n",
    "        json=data,\n",
    "        stream=True  # Enable streaming\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return\n",
    "    \n",
    "    # Process SSE stream\n",
    "    full_answer = \"\"\n",
    "    contexts = []\n",
    "    \n",
    "    print(\"üí¨ Streaming response:\\n\")\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line = line.decode('utf-8')\n",
    "            \n",
    "            # SSE format: \"data: {json}\"\n",
    "            if line.startswith(\"data: \"):\n",
    "                try:\n",
    "                    json_str = line[6:]  # Remove \"data: \" prefix\n",
    "                    event = json.loads(json_str)\n",
    "                    \n",
    "                    if event[\"status\"] == \"success\":\n",
    "                        data = event[\"data\"]\n",
    "                        \n",
    "                        # Check for message content\n",
    "                        if \"message\" in data and \"content\" in data[\"message\"]:\n",
    "                            content = data[\"message\"][\"content\"]\n",
    "                            print(content, end=\"\", flush=True)\n",
    "                            full_answer += content\n",
    "                        \n",
    "                        # Check for contexts\n",
    "                        if \"contexts\" in data:\n",
    "                            contexts = data[\"contexts\"]\n",
    "                        \n",
    "                        # Check if this is the final event\n",
    "                        if data.get(\"is_final_event\"):\n",
    "                            print(\"\\n\\n‚úÖ Stream complete\")\n",
    "                            break\n",
    "                \n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"\\n‚ö†Ô∏è Failed to parse: {line}\")\n",
    "    \n",
    "    return full_answer, contexts\n",
    "\n",
    "# Test streaming\n",
    "question = \"What are the benefits of using Storm API for document processing?\"\n",
    "print(f\"ü§î Question: {question}\\n\")\n",
    "\n",
    "answer, contexts = stream_chat(question)\n",
    "\n",
    "if contexts:\n",
    "    print(f\"\\nüìö Used {len(contexts)} sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Streaming Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingChatHandler:\n",
    "    \"\"\"Advanced handler for streaming chat responses.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, api_url=\"https://https://live-stargate.sionic.im\"):\n",
    "        self.api_key = api_key\n",
    "        self.api_url = api_url\n",
    "        self.headers = {\"storm-api-key\": api_key}\n",
    "    \n",
    "    def parse_sse_event(self, line):\n",
    "        \"\"\"Parse a Server-Sent Event line.\"\"\"\n",
    "        if not line or not line.startswith(\"data: \"):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            json_str = line[6:]  # Remove \"data: \" prefix\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    \n",
    "    def stream_with_callbacks(\n",
    "        self, \n",
    "        question, \n",
    "        on_token=None, \n",
    "        on_context=None, \n",
    "        on_complete=None,\n",
    "        bucket_ids=None,\n",
    "        thread_id=None\n",
    "    ):\n",
    "        \"\"\"Stream with callback functions for different events.\"\"\"\n",
    "        \n",
    "        data = {\n",
    "            \"question\": question,\n",
    "            \"bucketIds\": bucket_ids or []\n",
    "        }\n",
    "        \n",
    "        if thread_id:\n",
    "            data[\"threadId\"] = thread_id\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.api_url}/api/v2/answer/stream\",\n",
    "            headers=self.headers,\n",
    "            json=data,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"API Error {response.status_code}: {response.text}\")\n",
    "        \n",
    "        full_answer = \"\"\n",
    "        contexts = []\n",
    "        metadata = {}\n",
    "        \n",
    "        for line in response.iter_lines():\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            event = self.parse_sse_event(line.decode('utf-8'))\n",
    "            if not event or event.get(\"status\") != \"success\":\n",
    "                continue\n",
    "            \n",
    "            data = event[\"data\"]\n",
    "            \n",
    "            # Handle message content\n",
    "            if \"message\" in data and \"content\" in data[\"message\"]:\n",
    "                content = data[\"message\"][\"content\"]\n",
    "                full_answer += content\n",
    "                \n",
    "                if on_token:\n",
    "                    on_token(content)\n",
    "            \n",
    "            # Handle contexts\n",
    "            if \"contexts\" in data:\n",
    "                contexts = data[\"contexts\"]\n",
    "                if on_context:\n",
    "                    on_context(contexts)\n",
    "            \n",
    "            # Store metadata\n",
    "            if \"id\" in data:\n",
    "                metadata[\"chat_id\"] = data[\"id\"]\n",
    "            if \"task_id\" in data:\n",
    "                metadata[\"task_id\"] = data[\"task_id\"]\n",
    "            if \"model\" in data:\n",
    "                metadata[\"model\"] = data[\"model\"]\n",
    "            \n",
    "            # Check for completion\n",
    "            if data.get(\"is_final_event\"):\n",
    "                if on_complete:\n",
    "                    on_complete(full_answer, contexts, metadata)\n",
    "                break\n",
    "        \n",
    "        return full_answer, contexts, metadata\n",
    "\n",
    "# Create handler\n",
    "handler = StreamingChatHandler(API_KEY)\n",
    "\n",
    "# Define callbacks\n",
    "def on_token(token):\n",
    "    \"\"\"Called for each token received.\"\"\"\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "def on_context(contexts):\n",
    "    \"\"\"Called when contexts are received.\"\"\"\n",
    "    print(f\"\\n\\nüìö Received {len(contexts)} context sources\")\n",
    "\n",
    "def on_complete(answer, contexts, metadata):\n",
    "    \"\"\"Called when streaming is complete.\"\"\"\n",
    "    print(\"\\n\\n‚úÖ Streaming complete!\")\n",
    "    print(f\"üìä Metadata: {metadata}\")\n",
    "\n",
    "# Test with callbacks\n",
    "print(\"üéØ Streaming with callbacks:\\n\")\n",
    "\n",
    "try:\n",
    "    answer, contexts, metadata = handler.stream_with_callbacks(\n",
    "        \"Explain how Storm API handles document parsing\",\n",
    "        on_token=on_token,\n",
    "        on_context=on_context,\n",
    "        on_complete=on_complete\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Streaming UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a streaming UI component\n",
    "class StreamingUI:\n",
    "    \"\"\"Simulated UI for streaming responses.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.buffer = \"\"\n",
    "        self.tokens_received = 0\n",
    "        self.start_time = None\n",
    "    \n",
    "    def start_response(self):\n",
    "        \"\"\"Called when response starts.\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        print(\"üí≠ Thinking...\", end=\"\", flush=True)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Add a token to the display.\"\"\"\n",
    "        if self.tokens_received == 0:\n",
    "            print(\"\\rüí¨ \", end=\"\", flush=True)  # Clear \"Thinking...\"\n",
    "        \n",
    "        self.buffer += token\n",
    "        self.tokens_received += 1\n",
    "        print(token, end=\"\", flush=True)\n",
    "    \n",
    "    def show_typing_indicator(self):\n",
    "        \"\"\"Show typing animation.\"\"\"\n",
    "        indicators = [\"‚†ã\", \"‚†ô\", \"‚†π\", \"‚†∏\", \"‚†º\", \"‚†¥\", \"‚†¶\", \"‚†ß\", \"‚†á\", \"‚†è\"]\n",
    "        for i in range(10):\n",
    "            print(f\"\\r{indicators[i % len(indicators)]} Generating...\", end=\"\", flush=True)\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    def complete_response(self, contexts):\n",
    "        \"\"\"Called when response is complete.\"\"\"\n",
    "        elapsed = time.time() - self.start_time\n",
    "        \n",
    "        print(f\"\\n\\nüìä Response Stats:\")\n",
    "        print(f\"  ‚Ä¢ Tokens: {self.tokens_received}\")\n",
    "        print(f\"  ‚Ä¢ Time: {elapsed:.2f}s\")\n",
    "        print(f\"  ‚Ä¢ Speed: {self.tokens_received / elapsed:.1f} tokens/s\")\n",
    "        print(f\"  ‚Ä¢ Sources: {len(contexts)}\")\n",
    "\n",
    "# Example: Streaming with UI\n",
    "ui = StreamingUI()\n",
    "\n",
    "def stream_with_ui(question):\n",
    "    \"\"\"Stream response with UI updates.\"\"\"\n",
    "    \n",
    "    ui.start_response()\n",
    "    \n",
    "    try:\n",
    "        answer, contexts, metadata = handler.stream_with_callbacks(\n",
    "            question,\n",
    "            on_token=ui.add_token,\n",
    "            on_complete=lambda a, c, m: ui.complete_response(c)\n",
    "        )\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Streaming error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test streaming UI\n",
    "print(\"üñ•Ô∏è Streaming UI Demo:\\n\")\n",
    "question = \"What are the key features of Storm API's streaming endpoint?\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "\n",
    "answer = stream_with_ui(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handling Streaming Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_stream_chat(question, max_retries=3, timeout=60):\n",
    "    \"\"\"Stream with error handling and recovery.\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{API_URL}/api/v2/answer/stream\",\n",
    "                headers=headers,\n",
    "                json={\"question\": question, \"bucketIds\": []},\n",
    "                stream=True,\n",
    "                timeout=timeout\n",
    "            )\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå HTTP {response.status_code}: {response.text}\")\n",
    "                continue\n",
    "            \n",
    "            full_answer = \"\"\n",
    "            error_count = 0\n",
    "            last_event_time = time.time()\n",
    "            \n",
    "            for line in response.iter_lines():\n",
    "                # Check for timeout between events\n",
    "                if time.time() - last_event_time > 30:\n",
    "                    raise TimeoutError(\"No events for 30 seconds\")\n",
    "                \n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                last_event_time = time.time()\n",
    "                \n",
    "                try:\n",
    "                    line = line.decode('utf-8')\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        event = json.loads(line[6:])\n",
    "                        \n",
    "                        if event[\"status\"] == \"error\":\n",
    "                            print(f\"\\n‚ö†Ô∏è Stream error: {event}\")\n",
    "                            error_count += 1\n",
    "                            if error_count > 3:\n",
    "                                raise Exception(\"Too many stream errors\")\n",
    "                        \n",
    "                        elif event[\"status\"] == \"success\":\n",
    "                            data = event[\"data\"]\n",
    "                            \n",
    "                            if \"message\" in data and \"content\" in data[\"message\"]:\n",
    "                                content = data[\"message\"][\"content\"]\n",
    "                                print(content, end=\"\", flush=True)\n",
    "                                full_answer += content\n",
    "                            \n",
    "                            if data.get(\"is_final_event\"):\n",
    "                                print(\"\\n‚úÖ Success\")\n",
    "                                return full_answer\n",
    "                \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"\\n‚ö†Ô∏è Parse error: {e}\")\n",
    "                    error_count += 1\n",
    "            \n",
    "            # If we get here, stream ended without final event\n",
    "            print(\"\\n‚ö†Ô∏è Stream ended unexpectedly\")\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"\\n‚è±Ô∏è Timeout on attempt {attempt + 1}\")\n",
    "        \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"\\nüîå Connection error on attempt {attempt + 1}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error on attempt {attempt + 1}: {e}\")\n",
    "        \n",
    "        if attempt < max_retries - 1:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"‚è≥ Retrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test error handling\n",
    "print(\"üõ°Ô∏è Testing robust streaming:\\n\")\n",
    "\n",
    "test_cases = [\n",
    "    \"Normal question about Storm API\",\n",
    "    # Add more test cases as needed\n",
    "]\n",
    "\n",
    "for q in test_cases:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(\"A: \", end=\"\")\n",
    "    answer = robust_stream_chat(q)\n",
    "    if not answer:\n",
    "        print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Streaming Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìö Streaming Best Practices:\\n\")\n",
    "\n",
    "print(\"1. Connection Management:\")\n",
    "print(\"   ‚Ä¢ Set appropriate timeouts\")\n",
    "print(\"   ‚Ä¢ Handle connection drops gracefully\")\n",
    "print(\"   ‚Ä¢ Implement reconnection logic\")\n",
    "print(\"   ‚Ä¢ Monitor connection health\")\n",
    "\n",
    "print(\"\\n2. Error Handling:\")\n",
    "print(\"   ‚Ä¢ Parse SSE events safely\")\n",
    "print(\"   ‚Ä¢ Handle malformed events\")\n",
    "print(\"   ‚Ä¢ Implement retry logic\")\n",
    "print(\"   ‚Ä¢ Provide fallback options\")\n",
    "\n",
    "print(\"\\n3. UI/UX:\")\n",
    "print(\"   ‚Ä¢ Show typing indicators\")\n",
    "print(\"   ‚Ä¢ Display partial responses\")\n",
    "print(\"   ‚Ä¢ Update UI smoothly\")\n",
    "print(\"   ‚Ä¢ Handle long responses\")\n",
    "\n",
    "print(\"\\n4. Performance:\")\n",
    "print(\"   ‚Ä¢ Buffer tokens for smooth display\")\n",
    "print(\"   ‚Ä¢ Avoid blocking the UI thread\")\n",
    "print(\"   ‚Ä¢ Monitor memory usage\")\n",
    "print(\"   ‚Ä¢ Implement backpressure if needed\")\n",
    "\n",
    "# Example: Production-ready streaming class\n",
    "class ProductionStreamingClient:\n",
    "    \"\"\"Production-ready streaming client.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, config=None):\n",
    "        self.api_key = api_key\n",
    "        self.config = config or {\n",
    "            \"api_url\": \"https://https://live-stargate.sionic.im\",\n",
    "            \"timeout\": 60,\n",
    "            \"max_retries\": 3,\n",
    "            \"buffer_size\": 100,\n",
    "            \"event_timeout\": 30\n",
    "        }\n",
    "    \n",
    "    def stream(self, question, callbacks=None):\n",
    "        \"\"\"Stream with production features.\"\"\"\n",
    "        # Implementation would include:\n",
    "        # - Connection pooling\n",
    "        # - Event buffering\n",
    "        # - Metrics collection\n",
    "        # - Error recovery\n",
    "        # - Rate limiting\n",
    "        pass\n",
    "\n",
    "print(\"\\n‚úÖ Ready for production streaming!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned how to:\n",
    "- ‚úÖ Use Storm API's streaming endpoint\n",
    "- ‚úÖ Handle Server-Sent Events (SSE)\n",
    "- ‚úÖ Build streaming UI components\n",
    "- ‚úÖ Handle streaming errors\n",
    "- ‚úÖ Implement production-ready streaming\n",
    "- ‚úÖ Follow streaming best practices\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Context Search](./03-context-search.ipynb) - Search for relevant contexts\n",
    "- [Building a Chatbot](./04-chatbot-example.ipynb) - Complete chatbot with streaming\n",
    "- [Advanced Topics](../06-advanced-topics/02-webhooks.md) - Webhook integration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}